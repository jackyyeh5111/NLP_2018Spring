{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'oversold']"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"#oversold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u\"you're\",\n",
       " u\"you've\",\n",
       " u\"you'll\",\n",
       " u\"you'd\",\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u\"she's\",\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u\"it's\",\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u\"that'll\",\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u\"don't\",\n",
       " u'should',\n",
       " u\"should've\",\n",
       " u'now',\n",
       " u'd',\n",
       " u'll',\n",
       " u'm',\n",
       " u'o',\n",
       " u're',\n",
       " u've',\n",
       " u'y',\n",
       " u'ain',\n",
       " u'aren',\n",
       " u\"aren't\",\n",
       " u'couldn',\n",
       " u\"couldn't\",\n",
       " u'didn',\n",
       " u\"didn't\",\n",
       " u'doesn',\n",
       " u\"doesn't\",\n",
       " u'hadn',\n",
       " u\"hadn't\",\n",
       " u'hasn',\n",
       " u\"hasn't\",\n",
       " u'haven',\n",
       " u\"haven't\",\n",
       " u'isn',\n",
       " u\"isn't\",\n",
       " u'ma',\n",
       " u'mightn',\n",
       " u\"mightn't\",\n",
       " u'mustn',\n",
       " u\"mustn't\",\n",
       " u'needn',\n",
       " u\"needn't\",\n",
       " u'shan',\n",
       " u\"shan't\",\n",
       " u'shouldn',\n",
       " u\"shouldn't\",\n",
       " u'wasn',\n",
       " u\"wasn't\",\n",
       " u'weren',\n",
       " u\"weren't\",\n",
       " u'won',\n",
       " u\"won't\",\n",
       " u'wouldn',\n",
       " u\"wouldn't\"]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'pos_num'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmer.stem('POS_NUM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NTUSD-Fin Model\n",
    "with open(\"pretrained_model/NTUSD-Fin/NTUSD_Fin_word_v1.0.json\", 'r') as f:\n",
    "    fin_word = json.load(f)\n",
    "with open(\"pretrained_model/NTUSD-Fin/NTUSD_Fin_hashtag_v1.0.json\", 'r') as f:\n",
    "    fin_hashtag = json.load(f)\n",
    "\n",
    "fin_model = {}\n",
    "for fin in fin_word:\n",
    "    token = stemmer.stem(fin[\"token\"])\n",
    "    \n",
    "    fin_model[token] = fin\n",
    "for fin in fin_hashtag:\n",
    "    fin_model['#'+token] = fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5702130399167209"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_sentiment = 0.0\n",
    "for word in (fin_word + fin_hashtag):\n",
    "    market_sentiment += word[\"market_sentiment\"]\n",
    "\n",
    "market_sentiment / len(fin_word + fin_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([<type 'unicode'>])\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/training_set.json\", 'r') as f:\n",
    "    train_datas = json.load(f)\n",
    "\n",
    "a = []\n",
    "for data in train_datas:\n",
    "    a.append(type(data[\"tweet\"]))\n",
    "print set(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = u\"aaa\"\n",
    "type(s.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-13  eees  $coma'"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet = \"-13%  eee's  $coma\"\n",
    "snippet = re.sub(u\"(?:[^a-zA-Z0-9\\+\\-$ ])\", \"\", snippet)\n",
    "snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(datas):\n",
    "    texts = []\n",
    "    senti_scores = []\n",
    "    for data in datas:\n",
    "        \n",
    "        snippet = data[\"snippet\"]\n",
    "        \n",
    "        if type(snippet) == list:\n",
    "            snippet = ' '.join(snippet)\n",
    "        \n",
    "        elif snippet == [] or snippet == \"\": # some data have empty snippet\n",
    "            snippet = data[\"tweet\"]\n",
    "            \n",
    "        raw_snippet = snippet\n",
    "\n",
    "        # remove token\n",
    "        snippet = re.sub('https?://[A-Za-z0-9./]+', ' ', snippet)\n",
    "        snippet = re.sub(u\"(?:[^a-zA-Z0-9\\+\\-\\$'#\\/])\", ' ', snippet)\n",
    "        \n",
    "#         snippet = word_tokenize(snippet)\n",
    "        snippet = snippet.split(' ')\n",
    "    \n",
    "        # monitor\n",
    "        if snippet == []:\n",
    "            print raw_snippet\n",
    "\n",
    "        # remove <num>, <comp> \n",
    "#         snippet = re.sub(\"\\d+(?=\\s)\", \"<num>\", snippet)\n",
    "#         snippet = re.sub(r'\\s+\\+<num>?(?=\\s)', 'POS_NUM', snippet)\n",
    "#         snippet = re.sub(r'\\s+\\-<num>?(?=\\s)', 'NEG_NUM', snippet)\n",
    "        \n",
    "        snippet = [re.sub(\"\\d+.*\", \"<num>\", word) for word in snippet]         \n",
    "        snippet = [re.sub(r'\\+<num>.*', '_pos_num', word) for word in snippet]\n",
    "        snippet = [re.sub(r'\\-<num>.*', '_neg_num', word) for word in snippet]\n",
    "        snippet = [word for word in snippet if \"<num>\" not in word]\n",
    "        \n",
    "#         snippet = [word for word in snippet if word.startswith('http') == False]\n",
    "        snippet = [word for word in snippet if word.startswith('$') == False]\n",
    "    \n",
    "        # word_tokenize\n",
    "#         snippet = ' '.join(snippet).encode('utf-8')\n",
    "#         snippet = snippet.translate(None, string.punctuation)\n",
    "       \n",
    "#         snippet = ' '.join(snippet)\n",
    "#         snippet = word_tokenize(snippet)\n",
    "        \n",
    "        \n",
    "        # stopwords includes some important words like \"down\"\n",
    "#         snippet = [word for word in snippet if word not in stopwords.words('english')] # remove stopwords\n",
    "        \n",
    "        snippet = [stemmer.stem(word) for word in snippet] # stem words\n",
    "        snippet = [word for word in snippet if word != '' ]\n",
    "    \n",
    "#         print raw_snippet\n",
    "#         print snippet\n",
    "        \n",
    "#         print ' '.join(snippet)\n",
    "        \n",
    "#         print snippet\n",
    "#         print raw_snippet\n",
    "    \n",
    "        if snippet == []:\n",
    "            print \"->\" + raw_snippet\n",
    "  \n",
    "        # ignore empty training data\n",
    "        if snippet != []:\n",
    "            texts.append(snippet)\n",
    "            senti_scores.append(data[\"sentiment\"])\n",
    "        \n",
    "    return texts, senti_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1396"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->$YOKU 0\n",
      "->15%\n"
     ]
    }
   ],
   "source": [
    "texts, senti_scores = preprocess(train_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1394"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocab(texts, word_count_threshold=2):\n",
    "    # borrowed this function from NeuralTalk\n",
    "    print 'preprocessing word counts and creating vocab based on word count threshold %d' % (word_count_threshold, )\n",
    "\n",
    "    word_counts = {}\n",
    "\n",
    "    for text in texts:\n",
    "        for w in text:\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "    print \"after filter, vocab count: %d\"% len(vocab)\n",
    "\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapWordAndIdx(vocab):\n",
    "\n",
    "\tidx2word = {}\n",
    "\tidx2word[0] = '<pad>'\n",
    "\tidx2word[1] = '<bos>'\n",
    "\tidx2word[2] = '<eos>'\n",
    "\tidx2word[3] = '<unk>'\n",
    "\n",
    "\tword2idx = {}\n",
    "\tword2idx['<pad>'] = 0\n",
    "\tword2idx['<bos>'] = 1\n",
    "\tword2idx['<eos>'] = 2\n",
    "\tword2idx['<unk>'] = 3\n",
    "\n",
    "\tfor idx, w in enumerate(vocab):\n",
    "\t\tword2idx[w] = idx + 4\n",
    "\t\tidx2word[idx+4] = w\n",
    "\n",
    "\treturn word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing word counts and creating vocab based on word count threshold 2\n",
      "after filter, vocab count: 794\n"
     ]
    }
   ],
   "source": [
    "vocab = buildVocab(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx, idx2word = mapWordAndIdx(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "bos_idx = word2idx['<bos>']\n",
    "eos_idx = word2idx['<eos>']\n",
    "unk_idx = word2idx['<unk>']\n",
    "\n",
    "for text in texts:\n",
    "    #     x = [word2idx[word] if word in word2idx else unk_idx for word in text]\n",
    "    x = [word2idx[word] for word in text if word in word2idx]\n",
    "#     x = [bos_idx] + x + [eos_idx]\n",
    "    x_train.append(x)\n",
    "\n",
    "y_train = senti_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "np.save(\"data/x_train\", x_train)\n",
    "np.save(\"data/y_train\", y_train)\n",
    "\n",
    "with open(\"data/word2idx.json\", 'w') as f:\n",
    "    json.dump(word2idx, f)\n",
    "\n",
    "with open(\"data/idx2word.json\", 'w') as f:\n",
    "    json.dump(idx2word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test_set.json\", 'r') as f:\n",
    "    test_datas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts, test_senti_scores = preprocess(test_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "bos_idx = word2idx['<bos>']\n",
    "eos_idx = word2idx['<eos>']\n",
    "unk_idx = word2idx['<unk>']\n",
    "\n",
    "for text in test_texts:\n",
    "#     x = [word2idx[word] if word in word2idx else unk_idx for word in text]\n",
    "    x = [word2idx[word] for word in text if word in word2idx]\n",
    "#     x = [bos_idx] + x + [eos_idx]\n",
    "    x_test.append(x)\n",
    "\n",
    "y_test = test_senti_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "np.save(\"data/x_test\", x_test)\n",
    "np.save(\"data/y_test\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.319000 <--> -0.318709\n",
      "[u'alot', u'to', u'worri', u'about']\n",
      "{u'snippet': u'alot to worry about', u'tweet': u'@MattKelmon 4 more years will be alot to worry about $spy $dia', u'target': u'$SPY', u'sentiment': -0.319}\n",
      "\n",
      "-0.389000 <--> -0.389600\n",
      "[u'anoth', u'sell', u'rate']\n",
      "{u'snippet': u'Another Sell Rating', u'tweet': u'Isolationism? Another Sell Rating, Sell Ratings On SPY, QQQ, IWM And Several Companies https://t.co/6NKWEcrBA5 $AAPL $AXP $BAC $BLUE $FXE', u'target': u'$AXP', u'sentiment': -0.389}\n",
      "\n",
      "-0.389000 <--> -0.389600\n",
      "[u'anoth', u'sell', u'rate']\n",
      "{u'snippet': u'Another Sell Rating', u'tweet': u'Isolationism? Another Sell Rating, Sell Ratings On SPY, QQQ, IWM And Several Companies https://t.co/6NKWEcrBA5 $AAPL $AXP $BAC $BLUE $FXE', u'target': u'$BAC', u'sentiment': -0.389}\n",
      "\n",
      "-0.331000 <--> -0.329954\n",
      "[u'bad', u'govern', u'not', u'confid', u'in', u'core', u'biz']\n",
      "{u'snippet': [u'Bad governance', u'not confident in core biz'], u'tweet': u'@jay_21_ maybe $YHOO or $BABA. Bad governance, not confident in core biz', u'target': u'$YHOO', u'sentiment': -0.331}\n",
      "\n",
      "-0.331000 <--> -0.329954\n",
      "[u'bad', u'govern', u'not', u'confid', u'in', u'core', u'biz']\n",
      "{u'snippet': [u'Bad governance', u'not confident in core biz'], u'tweet': u'@jay_21_ maybe $YHOO or $BABA. Bad governance, not confident in core biz', u'target': u'$BABA', u'sentiment': -0.331}\n",
      "\n",
      "0.334000 <--> 0.335079\n",
      "[u'higher']\n",
      "{u'snippet': [u'$STX', u'Higher'], u'tweet': u'Watching $STX $TOL $LEI Higher, $HTZ $MDXG $INSY lower. and maybe fade the metals gap.', u'target': u'$STX', u'sentiment': 0.334}\n",
      "\n",
      "-0.339000 <--> -0.340328\n",
      "[u'new', u'squeez', u'play']\n",
      "{u'snippet': u'new squeeze plays', u'tweet': u'new squeeze plays #stocks http://stks.co/t0nzL $XRS $SWKS $PCRX', u'target': u'$XRS', u'sentiment': -0.339}\n",
      "\n",
      "-0.339000 <--> -0.340328\n",
      "[u'new', u'squeez', u'play']\n",
      "{u'snippet': u'new squeeze plays', u'tweet': u'new squeeze plays #stocks http://stks.co/t0nzL $XRS $SWKS $PCRX', u'target': u'$PCRX', u'sentiment': -0.339}\n",
      "\n",
      "-0.188000 <--> -0.186442\n",
      "[u'as', u'for', u'no', u'word']\n",
      "{u'snippet': u'As for $TWTR, no words.', u'tweet': u'$FB is taking the shine off $AAPL and $GOOG. As for $TWTR, no words.', u'target': u'$TWTR', u'sentiment': -0.188}\n",
      "\n",
      "-0.221000 <--> -0.222611\n",
      "[u'out', u'for', u'now']\n",
      "{u'snippet': u'Out $BIDU for now', u'tweet': u'Out $BIDU for now.  Clearing some cash.  Will look for re-entry next week.', u'target': u'$BIDU', u'sentiment': -0.221}\n",
      "\n",
      "0.407000 <--> 0.405096\n",
      "[u'head', u'to', u'all', u'time', u'high']\n",
      "{u'snippet': u'Headed to all time highs', u'tweet': u'$ISRG What a move from 502 in Feb - Headed to all time highs. #Risk is some unforeseen safety issue with devices.', u'target': u'$ISRG', u'sentiment': 0.407}\n",
      "\n",
      "-0.315000 <--> -0.316908\n",
      "[u'short', u'interest', u'or', u'not']\n",
      "{u'snippet': u'60% short interest or not', u'tweet': u'$FRPT this could be in teens again in one trading day, 60% short interest or not', u'target': u'$FRPT', u'sentiment': -0.315}\n",
      "\n",
      "-0.341000 <--> -0.338693\n",
      "[u'lower']\n",
      "{u'snippet': [u'$HTZ', u'lower'], u'tweet': u'Watching $STX $TOL $LEI Higher, $HTZ $MDXG $INSY lower. and maybe fade the metals gap.', u'target': u'$HTZ', u'sentiment': -0.341}\n",
      "\n",
      "-0.438000 <--> -0.435251\n",
      "[u'biggest', u'market', u'loser']\n",
      "{u'snippet': u'Biggest Market Losers', u'tweet': u'2\\u20e30\\u20e3Biggest Market Losers $VRX $CSTM $ZYNE $ENDP $ARLZ $PTX $BLCM $NVIV $SGY $LABU $WBAI $SID $CJES $MNK $ASPS $HZNP https://t.co/ekwqzpzxmJ', u'target': u'$BLCM', u'sentiment': -0.438}\n",
      "\n",
      "-0.438000 <--> -0.435251\n",
      "[u'biggest', u'market', u'loser']\n",
      "{u'snippet': u'Biggest Market Losers', u'tweet': u'2\\u20e30\\u20e3Biggest Market Losers $VRX $CSTM $ZYNE $ENDP $ARLZ $PTX $BLCM $NVIV $SGY $LABU $WBAI $SID $CJES $MNK $ASPS $HZNP https://t.co/ekwqzpzxmJ', u'target': u'$ENDP', u'sentiment': -0.438}\n",
      "\n",
      "-0.438000 <--> -0.435251\n",
      "[u'biggest', u'market', u'loser']\n",
      "{u'snippet': u'Biggest Market Losers', u'tweet': u'2\\u20e30\\u20e3Biggest Market Losers $VRX $CSTM $ZYNE $ENDP $ARLZ $PTX $BLCM $NVIV $SGY $LABU $WBAI $SID $CJES $MNK $ASPS $HZNP https://t.co/ekwqzpzxmJ', u'target': u'$CSTM', u'sentiment': -0.438}\n",
      "\n",
      "-0.438000 <--> -0.435251\n",
      "[u'biggest', u'market', u'loser']\n",
      "{u'snippet': u'Biggest Market Losers', u'tweet': u'2\\u20e30\\u20e3Biggest Market Losers $VRX $CSTM $ZYNE $ENDP $ARLZ $PTX $BLCM $NVIV $SGY $LABU $WBAI $SID $CJES $MNK $ASPS $HZNP https://t.co/ekwqzpzxmJ', u'target': u'$NVIV', u'sentiment': -0.438}\n",
      "\n",
      "-0.438000 <--> -0.435251\n",
      "[u'biggest', u'market', u'loser']\n",
      "{u'snippet': u'Biggest Market Losers', u'tweet': u'2\\u20e30\\u20e3Biggest Market Losers $VRX $CSTM $ZYNE $ENDP $ARLZ $PTX $BLCM $NVIV $SGY $LABU $WBAI $SID $CJES $MNK $ASPS $HZNP https://t.co/ekwqzpzxmJ', u'target': u'$HZNP', u'sentiment': -0.438}\n",
      "\n",
      "-0.438000 <--> -0.435251\n",
      "[u'biggest', u'market', u'loser']\n",
      "{u'snippet': u'Biggest Market Losers', u'tweet': u'2\\u20e30\\u20e3Biggest Market Losers $VRX $CSTM $ZYNE $ENDP $ARLZ $PTX $BLCM $NVIV $SGY $LABU $WBAI $SID $CJES $MNK $ASPS $HZNP https://t.co/ekwqzpzxmJ', u'target': u'$CJES', u'sentiment': -0.438}\n",
      "\n",
      "0.415000 <--> 0.411805\n",
      "[u'buy', u'these', u'stock', u'while', u'there', u\"'s\", u'blood', u'in', u'the', u'street']\n",
      "{u'snippet': u\"Buy these 3 stocks while there's blood in the streets\", u'tweet': u\"Buy these 3 stocks while there's blood in the streets: https://t.co/J9RZkvZNeU $NXPI $IBM $LGF https://t.co/iDev2d4rfL\", u'target': u'$LGF', u'sentiment': 0.415}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "score_sort = np.load('score_sort.npy')\n",
    "pred = np.load('pred.npy')\n",
    "\n",
    "sort_idx = score_sort.reshape([-1,]).argsort()\n",
    "for idx in sort_idx[:20]:\n",
    "    print \"%f <--> %f\" % (y_test[idx], pred[idx])\n",
    "    print test_texts[idx]\n",
    "    print test_datas[idx]\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.361000 <--> -0.281642\n",
      "[u'sell', u'short', u'posit', u'on']\n",
      "{u'snippet': u'Sell Short Position on', u'tweet': u'$CNP Sell Short Position on CNP,...Closed for Profit $ 59,367.00 (7.75%) http://stks.co/ghjc', u'target': u'$CNP', u'sentiment': 0.361}\n",
      "\n",
      "0.543000 <--> -0.102388\n",
      "[u'oil', u'back', u'abov']\n",
      "{u'snippet': u'oil back above 60', u'tweet': u'$WRES moves better w/ oil anyways, oil back above 60', u'target': u'$WRES', u'sentiment': 0.543}\n",
      "\n",
      "-0.193000 <--> 0.452947\n",
      "[u'indic', u'are', u'ralli', u'today', u'a', u'lot', u'of', u'the', u'high', u'cover', u'compani', u\"'\", u'are', u'underperform']\n",
      "{u'snippet': [u'Indices are rallying today', u\"a lot of the 'highly covered companies' are underperforming\"], u'tweet': u'Indices are rallying today, but it seems like a lot of the \"highly covered companies\" are underperforming $AAPL $FB $AMZN $MCD $SPY', u'target': u'$AMZN', u'sentiment': -0.193}\n",
      "\n",
      "-0.329000 <--> 0.332659\n",
      "[u'bullshit', u'!', u'all', u'#', u'applestor', u'are', u'empti', u'in', u'#', u'china']\n",
      "{u'snippet': u'Bullshit! All #AppleStores are EMPTY in #China', u'tweet': u'Bullshit! All #AppleStores are EMPTY in #China $MS must be preparing to liquidate $AAPL https://t.co/mvqXVS1JE8  https://t.co/OzHNY1cJ8P', u'target': u'$AAPL', u'sentiment': -0.329}\n",
      "\n",
      "-0.380000 <--> 0.284487\n",
      "[u'report', u':', u'appl', u'sign', u'up', u'for', u'googl', u\"'s\", u'cloud', u'use', u'much', u'less', u'of', u'amazon']\n",
      "{u'snippet': [u\"Report: Apple signs up for Google's cloud\", u'uses much less of Amazon'], u'tweet': u\"Report: Apple signs up for Google's cloud, uses much less of Amazon's $AAPL $GOOG $GOOGL $AMZN $DROPB https://t.co/zN3KDGYvGT\", u'target': u'$AMZN', u'sentiment': -0.38}\n",
      "\n",
      "-0.334000 <--> 0.342777\n",
      "[u'took', u'my', u'proprietari', u'profit', u'exit', u'did', u\"n't\", u'like', u'the', u'option', u'liquid', u'ousi', u'spread']\n",
      "{u'snippet': [u'took my proprietary profit exit', u\"didn't like the option liquidity\", u'ousy spreads'], u'tweet': u'$AXDX I got bored, and took my proprietary profit exit. I didn&#39;t like the option liquidity and lousy spreads.', u'target': u'$AXDX', u'sentiment': -0.334}\n",
      "\n",
      "-0.391000 <--> 0.286934\n",
      "[u'i', u'love', u'blood', u'like', u'a', u'vampirekeep', u'bleed', u'for', u'me']\n",
      "{u'snippet': u'I Love blood like a vampire.....Keep bleeding for me', u'tweet': u'$NFLX I Love blood like a vampire.....Keep bleeding for me', u'target': u'$NFLX', u'sentiment': -0.391}\n",
      "\n",
      "0.399000 <--> -0.288007\n",
      "[u'bmw', u'enter', u'into', u'ride-shar', u'market', u'with']\n",
      "{u'snippet': u'BMW entering into ride-share market with i3', u'tweet': u'BMW entering into ride-share market with i3 https://t.co/TTB3L7Z5yT $TSLA $BAMXY', u'target': u'$BAMXY', u'sentiment': 0.399}\n",
      "\n",
      "-0.519000 <--> 0.193148\n",
      "[u'if', u'break', u'we', u'see', u'then']\n",
      "{u'snippet': u'if $249.84 breaks we see $245 then $240', u'tweet': u'$TSLA if $249.84 breaks we see $245 then $240', u'target': u'$TSLA', u'sentiment': -0.519}\n",
      "\n",
      "0.415000 <--> -0.298300\n",
      "[u'get', u'into', u'and']\n",
      "{u'snippet': u'get into $WFM and $MJN', u'tweet': u'@cek_cpa @ryanwallace198 @financialbuzz Guys - get into $WFM and $MJN.', u'target': u'$WFM', u'sentiment': 0.415}\n",
      "\n",
      "0.586000 <--> -0.136465\n",
      "[u'post', u'percent', u'jump', u'in', u'profit', u'steep', u'global', u'sale', u'increas', u'top', u'wall', u'st', u'view']\n",
      "{u'snippet': [u'posts 60 percent jump in 4Q profit', u'steep global sales increase', u'tops Wall St. view'], u'tweet': u'AP: Caterpillar posts 60 percent jump in 4Q profit on steep global sales increase, tops Wall St. view &gt; $CAT', u'target': u'$CAT', u'sentiment': 0.586}\n",
      "\n",
      "-0.488000 <--> 0.256083\n",
      "[u'retreat', u'like', u'clockwork']\n",
      "{u'snippet': u'retreats like clockwork', u'tweet': u'RT @joemccann the correleation between the dollar index and $SPY is simply amazing. $DX finds support, $SPY retreats like clockwork', u'target': u'$SPY', u'sentiment': -0.488}\n",
      "\n",
      "0.623000 <--> -0.143121\n",
      "[u'nobodi', u'is', u'recommend', u'the', u'sale', u'of', u'this', u'stock', u'big', u'thing', u'come']\n",
      "{u'snippet': [u'Nobody is recommending the sale of this stock', u'Big things coming'], u'tweet': u'Nobody is recommending the sale of this stock. Big things coming. $MU Micron https://t.co/ugUs7uVb5e', u'target': u'$MU', u'sentiment': 0.623}\n",
      "\n",
      "0.504000 <--> -0.273653\n",
      "[u'break-out', u'thru', u'&', u'dma', u'short', u'squeez', u'in', u'progress']\n",
      "{u'snippet': [u'Break-out thru 50 & 200- DMA', u'Short squeeze in progress'], u'tweet': u'#LongPos $TSLA 256 Break-out thru 50 &amp; 200- DMA (197-230) upper head res (274-279) Short squeeze in progress\\nNr term obj: 310\\nStop loss:239', u'target': u'$TSLA', u'sentiment': 0.504}\n",
      "\n",
      "0.418000 <--> -0.364817\n",
      "[u'rsi', u'is', u'down', u'at']\n",
      "{u'snippet': u'RSI is down at 20', u'tweet': u'$amzn 4 period RSI is down at 20, just read Connors book on trading strategies', u'target': u'$AMZN', u'sentiment': 0.418}\n",
      "\n",
      "0.562000 <--> -0.256981\n",
      "[u'the', u'top', u'brand', u'for', u'#', u'millenni', u'-', u'is', u'and', u'is', u'way', u'off', u'the', u'list', u'at']\n",
      "{u'snippet': u'The Top 10 brands for #millennials - $AAPL is #1, and $FB is way off the list at #65.', u'tweet': u'The Top 10 brands for #millennials - $AAPL is #1, and $FB is way off the list at #65. More: https://t.co/2IrrEpup7O https://t.co/HK94IURE4S', u'target': u'$AAPL', u'sentiment': 0.562}\n",
      "\n",
      "0.561000 <--> -0.277535\n",
      "[u'at', u'earn', u'is', u'a', u'steal']\n",
      "{u'snippet': u'At 11x earnings, $AAPL is a steal', u'tweet': u'Totally agree - At 11x earnings, $AAPL is a steal. @jimcramer', u'target': u'$AAPL', u'sentiment': 0.561}\n",
      "\n",
      "0.497000 <--> -0.363420\n",
      "[u'receiv', u'contract']\n",
      "{u'snippet': u'Receives 7-Year, $50M Contract', u'tweet': u'$HSC Receives 7-Year, $50M Contract at New Saudi Arabian Steel Mill', u'target': u'$HSC', u'sentiment': 0.497}\n",
      "\n",
      "-0.355000 <--> 0.586958\n",
      "[u'valuat', u'is', u'high', u'no', u'net', u'cash', u'fwd', u'growth', u'high', u'peg', u'histor', u'high', u'pe', u'&', u'fwd', u'pe']\n",
      "{u'snippet': [u'valuation is high', u'No net cash', u'fwd growth <10%', u'high peg', u'historical high pe & fwd PE'], u'tweet': u'Agree the $sbux valuation is high. No net cash, fwd growth &lt;10%, high peg, historical high pe &amp; fwd PE.', u'target': u'$SBUX', u'sentiment': -0.355}\n",
      "\n",
      "-0.602000 <--> 0.458645\n",
      "[u'be', u'su', u'by', u'ft', u'nobodi', u'will', u'buy', u'them', u'with', u'that', u'hang', u'over', u'their', u'head']\n",
      "{u'snippet': [u'$ENDP being sued by FT', u'nobody will buy them with that hanging over their heads'], u'tweet': u'$ENDP being sued by FTC.....nobody will buy them with that hanging over their heads', u'target': u'$ENDP', u'sentiment': -0.602}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx in sort_idx[-20:]:\n",
    "    print \"%f <--> %f\" % (y_test[idx], pred[idx])\n",
    "    print test_texts[idx]\n",
    "    print test_datas[idx]\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
